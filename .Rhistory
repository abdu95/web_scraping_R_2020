library(ggthemes)
clean_file <- "Documents/Egyetem/CEU/Teaching_2020/Coding_with_R/git_coding_1/ECBS-5208-Coding-1-Business-Analytics/Class_8/data/clean/WDI_lifeexp_clean.csv"
df <- read_csv(clean_file)
clean_file <- "D:/CEU/1 semester/coding_1_data-management-and-analysis_with_r/ECBS-5208-Coding-1-Business-Analytics/Class_9/data/clean/WDI_lifeexp_clean.csv"
df <- read_csv(clean_file)
clean_file <- "D:/CEU/1 semester/coding_1_data-management-and-analysis_with_r/ECBS-5208-Coding-1-Business-Analytics/Class_9/data/clean/WDI_co2_clean.csv"
df <- read_csv(clean_file)
df %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value)) +
facet_wrap(~key, scales = "free") +
geom_histogram()+
theme_wsj() +
scale_fill_wsj()
summary( df )
df <- df %>% mutate( gdptot = gdppc*population )
View(df)
# 1) co2 - gdptot: level-level model without scaling
ggplot( df , aes(x = gdptot, y = co2emission)) +
geom_point() +
geom_smooth(method="loess")+
labs(x = "Total GDP (2000 int. const. $, PPP )",y = "CO2 emission (years)")
# You can change the scale for Total GDP for checking log-transformation
ggplot( df , aes(x = gdptot, y = co2emission)) +
geom_point() +
geom_smooth(method="loess") +
labs(x = "Total GDP (2000 int. const. $, PPP, ln scale )",y = "CO2 emission (years)") +
scale_x_continuous( trans = log_trans(),  breaks = c(1,2,5,10,20,50,100,200,500,1000,10000) )
# You can change the scale for Total GDP and life-expectancy for checking log-transformation
ggplot( df , aes(x = gdptot, y = co2emission)) +
geom_point() +
geom_smooth(method="loess") +
labs(x = "Total GDP (2000 int. const. $, PPP, ln scale )",y = "CO2 emission (years, ln scale)") +
scale_x_continuous( trans = log_trans(),  breaks = c(1,2,5,10,20,50,100,200,500,1000,10000) ) +
scale_y_continuous( trans = log_trans() )
# 2) lifeexp - gdppc: level-level model without scaling
ggplot( df , aes(x = gdppc, y = co2emission)) +
geom_point() +
geom_smooth(method="loess")+
labs(x = "GDP/capita (2018 int. const. $, PPP )",y = "CO2 emission  (years)")
# 1) co2 - gdptot: level-level model without scaling
ggplot( df , aes(x = gdptot, y = co2emission)) +
geom_point() +
geom_smooth(method="loess")+
labs(x = "Total GDP (2000 int. const. $, PPP )",y = "CO2 emission (years)")
# You can change the scale for Total GDP for checking log-transformation
ggplot( df , aes(x = gdptot, y = co2emission)) +
geom_point() +
geom_smooth(method="loess") +
labs(x = "Total GDP (2000 int. const. $, PPP, ln scale )",y = "CO2 emission (years)") +
scale_x_continuous( trans = log_trans(),  breaks = c(1,2,5,10,20,50,100,200,500,1000,10000) )
# You can change the scale for Total GDP and life-expectancy for checking log-transformation
ggplot( df , aes(x = gdptot, y = co2emission)) +
geom_point() +
geom_smooth(method="loess") +
labs(x = "Total GDP (2000 int. const. $, PPP, ln scale )",y = "CO2 emission (years, ln scale)") +
scale_x_continuous( trans = log_trans(),  breaks = c(1,2,5,10,20,50,100,200,500,1000,10000) ) +
scale_y_continuous( trans = log_trans() )
# You can change the scale for GDP/capita for checking log-transformation
ggplot( df , aes(x = gdppc, y = co2emission)) +
geom_point() +
geom_smooth(method="loess")+
labs(x = "GDP/capita (2000 int. const. $, PPP , ln scale )",y = "CO2 emission (years)") +
scale_x_continuous( trans = log_trans(), breaks = seq(0, 120, by = 20))
# You can change the scale for GDP/capita and life-expectancy for checking log-transformation
ggplot( df , aes(x = gdppc, y = co2emission ))  +
geom_point() +
geom_smooth(method="loess")+
labs(x = "GDP/capita (2000 int. const. $, PPP , ln scale )",y = "CO2 emission (years, ln scale)") +
scale_x_continuous( trans = log_trans(), breaks = seq(0, 120, by = 20))+
scale_y_continuous( trans = log_trans() )
View(df)
#Looking better: parabola, convex, reverted
# Ati: This has better than simple linear regression
# Cool: Ati had HTML Conclusion table like Agoston did last time
# You can change the scale for GDP/capita and life-expectancy for checking log-transformation
ggplot( df , aes(x = gdppc, y = co2emission ))  +
geom_point() +
geom_smooth(method="loess")+
labs(x = "GDP/capita (2000 int. const. $, PPP , ln scale )",y = "CO2 emission (years, ln scale)") +
scale_x_continuous( trans = log_trans(), breaks = seq(0, 120, by = 20))+
scale_y_continuous( trans = log_trans() )
library(rvest)
library(data.table)
rm(list=ls())
#read_html just pass the url
t <- read_html('https://www.vox.com/search?page=1&q=Biden')
# save as html file
write_html(t, 't.html')
getwd()
rm(t)
file_html
# to get a web page, pass URL to read_html
file_html <- read_html('https://www.vox.com/search?page=1&q=Biden')
View(file_html)
# save as html file
write_html(file_html, 'index.html')
headings <-
file_html %>%
html_nodes('.c-entry-box--compact') %>%
html_text()
headings <-
file_html %>%
html_nodes('.c-compact-river__entry') %>%
html_text()
headings
headings <-
file_html %>%
html_nodes('.c-entry-box--compact__title') %>%
html_text()
headings
posted_date <-
file_html %>%
html_nodes('.c-byline__item') %>%
html_text()
posted_date <-
file_html %>%
html_nodes('time') %>%
html_text()
posted_date
dates <- file_html %>%
html_nodes('time') %>%
html_attrs() %>%
map(1) %>%
# Parse the string into a datetime object with lubridate
ymd_hms() %>%
unlist()
posted_date <-
file_html %>%
html_nodes('span .c-byline__item') %>%
html_text() %>%
as.numeric()
posted_date
posted_date <-
file_html %>%
html_nodes('.c-byline__item span') %>%
html_text() %>%
as.numeric()
posted_date
posted_date <-
file_html %>%
html_nodes('.c-byline__item span') %>%
html_text()
posted_date
author <-
file_html %>%
html_nodes('.c-byline__item span') %>%
html_text()
author
author <-
file_html %>%
html_nodes('c-byline__author-name') %>%
html_text()
author <-
file_html %>%
html_nodes('.c-byline__author-name') %>%
html_text()
author
author <-
file_html %>%
html_nodes('.c-byline__item') %>%
html_text()
author <-
file_html %>%
html_node('.c-byline__item') %>%
html_text()
author <-
file_html %>%
html_nodes('.c-byline__item') %>%
html_text()
author <-
file_html %>%
html_nodes('.c-byline__author-name') %>%
html_text()
author <-
file_html %>%
html_nodes('.c-byline__item') %>%
html_text()
author
author <-
file_html %>%
html_nodes('.c-byline__item') %>%
html_text()
gsub("[\n]", "", author)
gsub("[\n]", "", author)
author
what <- gsub("[\n]", "", author)
what
author <-
file_html %>%
html_nodes('.c-byline__item') %>%
html_text() %>%
gsub("[\n]", "", author)
author <-
file_html %>%
html_nodes('.c-byline__item') %>%
html_text()
author <- gsub("[\n]", "", author)
rm(what)
author
x <- "  Some text. "
x <- trimws(x)
author <- trimws(author)
rm(x)
author
author <-
file_html %>%
html_nodes('.c-byline__item') %>%
[[1]]
author <-
file_html %>%
html_nodes('.c-byline__item') %>%
.[[1]] %>%
html_text()
author <-
file_html %>%
html_nodes('.c-byline__item') %>%
.[[2]] %>%
html_text()
author <-
file_html %>%
html_nodes('.c-byline__item') %>%
html_text()
author
author <-
file_html %>%
html_nodes('.c-byline__item') %>%
html_text(trim = TRUE)
author <-
file_html %>%
html_nodes('.c-byline__item') %>%
html_text()
author
author <-
file_html %>%
html_nodes('.c-byline__item') %>%
html_text(trim = TRUE)
author
author <-
file_html %>%
html_nodes('.c-byline__item') %>%
html_text(trim = TRUE)
author
file_html
results <- file_html %>% html_nodes('.c-byline__item')
View(results)
View(results)
View(results)
results
author
results[2]
first_result %>% html_nodes("time")
first_result <- results[2]
first_result %>% html_nodes("time")
date <- first_result %>% html_nodes("time") %>% html_text()
trim = TRUE
rm(trim)
date <- first_result %>% html_nodes("time") %>% html_text(trim = TRUE)
rm(first_result)
second_result <- results[2]
second_result %>% html_nodes("time")
date <- second_result %>% html_nodes("time") %>% html_text(trim = TRUE)
xml_contents(second_result)
times <-
file_html %>%
html_nodes('time') %>%
html_text()
times <-
file_html %>%
html_nodes('time') %>%
html_text(trim = TRUE)
posted_date <-
file_html %>%
html_nodes('time') %>%
html_text(trim = TRUE)
posted_date
author
xml_contents(second_result)
author <-
file_html %>%
html_nodes('.c-byline__item') %>%
html_text(trim = TRUE)
author
for(x in seq(1,length(author),3)) {
print(x)
}
print(author[x])
for(x in seq(1,length(author),3)) {
print(author[x])
}
authors <- c()
rm(x)
for(x in seq(1,length(author),3)) {
authors <- c(authors, author[x])
}
authors
times
rm(times)
demo <-
file_html %>%
html_nodes('.c-entry-box--compact__title')
demo
View(demo)
View(demo)
first_result <- demo[1]
first_result %>% html_nodes('a')
first_result %>% html_nodes('a')
first_result %>% html_nodes('a') %>% html_attr('href')
link <- first_result %>% html_nodes('a') %>% html_attr('href')
demo <-
file_html %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_attr('href')
demo
post_links <-
file_html %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_attr('href')
rm(demo)
rm(link)
date
rm(date)
# to create a df
df <- data.frame('headings' = headings, 'links' = post_links, 'date' = posted_date, 'author' = authors)
View(df)
# to create a df
df <- data.frame('headings' = headings, 'post_link' = post_links, 'date' = posted_date, 'author' = authors)
View(df)
rm(first_result)
get_vox_biden_page <- function(arg_url) {
file_html <- read_html(arg_url)
# save it as html document
write_html(file_html, 'html_file.html')
headings <-
file_html %>%
html_nodes('.c-entry-box--compact__title') %>%
html_text()
post_links <-
file_html %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_attr('href')
posted_date <-
file_html %>%
html_nodes('time') %>%
html_text(trim = TRUE)
author <-
file_html %>%
html_nodes('.c-byline__item') %>%
html_text(trim = TRUE)
authors <- c()
for(x in seq(1,length(author),3)) {
authors <- c(authors, author[x])
}
df <- data.frame('headings' = headings, 'post_link' = post_links, 'date' = posted_date, 'author' = authors)
return(df)
}
# test the function
test_df <- get_vox_biden_page('https://www.vox.com/search?page=1&q=Biden')
View(test_df)
# test the function
test_df <- get_vox_biden_page('https://www.vox.com/search?page=2&q=Biden')
View(test_df)
View(test_df)
# test the function
test_df <- get_vox_biden_page('https://www.vox.com/search?page=3&q=Biden')
View(test_df)
# test the function
test_df <- get_vox_biden_page('https://www.vox.com/search?page=1&q=Biden')
View(test_df)
#apply a function to a list
df_list <- lapply(test_urls, get_vox_biden_page)
# generate the URLs
test_urls <- paste0('https://www.vox.com/search?page=', 2:5, '&q=Biden' )
#apply a function to a list
df_list <- lapply(test_urls, get_vox_biden_page)
test_urls
# generate the URLs
test_urls <- paste0('https://www.vox.com/search?page=', 1:5, '&q=Biden' )
#apply a function to a list
df_list <- lapply(test_urls, get_vox_biden_page)
test_urls
View(df_list)
#create a data.table object of the list of data.frames
df <- rbindlist(df_list)
View(df)
main_url <- 'https://www.vox.com/search?page='
page = 1
keyword = 'Biden'
result <- paste0(main_url, page, '&q=', keyword)
rm(main_url)
rm(test_urls)
rm(keyword)
keyword = 'Biden'
no_pages = 5
test_urls <- paste0('https://www.vox.com/search?page=', 1:no_pages, '&q=', keyword)
test_urls
rm(test_urls)
rm(keyword)
rm(no_pages)
rm(result)
rm(page)
library(data.table)
# a function which downloads information from the website to dataframe
get_one_page_from_vox <- function(arg_url) {
# to get a web page, pass URL to read_html
file_html <- read_html(arg_url)
# save it as html document
write_html(file_html, 'html_file.html')
headings <-
file_html %>%
html_nodes('.c-entry-box--compact__title') %>%
html_text()
post_links <-
file_html %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_attr('href')
posted_date <-
file_html %>%
html_nodes('time') %>%
html_text(trim = TRUE)
author <-
file_html %>%
html_nodes('.c-byline__item') %>%
html_text(trim = TRUE)
authors <- c()
for(x in seq(1,length(author),3)) {
authors <- c(authors, author[x])
}
# to create a dataframe
df <- data.frame('headings' = headings, 'post_link' = post_links, 'date' = posted_date, 'author' = authors)
return(df)
}
rm(get_vox_biden_page)
#apply a function to a list
df_list <- lapply(test_urls, get_one_page_from_vox)
# generate the URLs
test_urls <- paste0('https://www.vox.com/search?page=', 1:5, '&q=Biden' )
#apply a function to a list
df_list <- lapply(test_urls, get_one_page_from_vox)
rm(test_urls)
get_one_page_from_techworld  <- function(my_url) {
print(my_url)
t <- read_html(my_url)
boxes <- t %>%
html_nodes('.shadow')
x <- boxes[[1]]
boxes_df <- lapply(boxes, function(x){
t_list <- list()
t_list[['title']] <- x %>% html_nodes('.searchtitle') %>% html_text()
t_list[['link']] <- x %>% html_nodes('.searchtitle') %>% html_attr('href')
t_list[['teaser']] <-  paste0(x %>% html_nodes('p') %>% html_text(), collapse = ' ')
return(data.frame(t_list))
})
df <- rbindlist(boxes_df)
return(df)
}
my_url <- 'https://www.technewsworld.com/perl/search.pl?x=16&y=10&query=big+data'
result_df <- get_one_page_from_techworld(my_url)
View(result_df)
# test the function
test_df <- get_one_page_from_vox('https://www.vox.com/search?page=1&q=Biden')
View(test_df)
get_vox_pages <- function(keyword, no_pages){
main_url <- 'https://www.vox.com/search?page='
# generate the URLs
links <- paste0(main_url, 1:no_pages, '&q=', keyword)
return_df <- rbindlist(lapply(links, get_one_page_from_vox))
}
returned_df <- get_vox_pages(keyword = 'Biden', 6)
View(returned_df)
getwd()
setwd('D:\CEU\1 semester\web-scraping-R\web_scraping_R_2020')
setwd('D:/CEU/1 semester/web-scraping-R/web_scraping_R_2020')
getwd()
get_vox_pages <- function(keyword, no_pages){
main_url <- 'https://www.vox.com/search?page='
# generate the URLs
links <- paste0(main_url, 1:no_pages, '&q=', keyword)
return_df <- rbindlist(lapply(links, get_one_page_from_vox))
write_csv(return_df, 'data/result.csv')
return(return_df)
}
get_vox_pages <- function(keyword, no_pages){
main_url <- 'https://www.vox.com/search?page='
# generate the URLs
links <- paste0(main_url, 1:no_pages, '&q=', keyword)
return_df <- rbindlist(lapply(links, get_one_page_from_vox))
write_csv(return_df, 'data/result.csv')
saveRDS(return_df, file = 'data/rdsfile.rds')
return(return_df)
}
# check the function
returned_df <- get_vox_pages(keyword = 'Biden', 6)
#' Create a function which requires two arguments.
#' First a keyword then a number of pages to download.
library(readr)
get_vox_pages <- function(keyword, no_pages){
main_url <- 'https://www.vox.com/search?page='
# generate the URLs
links <- paste0(main_url, 1:no_pages, '&q=', keyword)
return_df <- rbindlist(lapply(links, get_one_page_from_vox))
write_csv(return_df, 'data/result.csv')
saveRDS(return_df, file = 'data/rdsfile.rds')
return(return_df)
}
# check the function
returned_df <- get_vox_pages(keyword = 'Biden', 6)
